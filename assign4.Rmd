---
output:
  rmarkdown: github_document
  html_document: default
  pdf_document: default
---


#### Assignment 4: Recommendations

This assign focuses on ecommendations using generated data. The following code generates a set of 20 users and a set of 20 user-items:
```{r}
library(recommenderlab, quietly = TRUE)
set.seed(1)
m <- matrix(sample(c(as.numeric(1:5), NA), 400,
                   replace=TRUE, prob=c(0.05, 0.1, 0.15, 0.1, 0.05, 0.55)),
                   ncol=20,
                   dimnames=list(user=paste("u", 1:20, sep = ""),
                                 item=paste("i", 1:20, sep = "")))
head(m)

# The rating matrix can be written externally as a CSV file.
# write.csv(m, "m.csv")
# m.df <- read.csv("m.csv", header = TRUE, row.names = 1)
# head(as.matrix(m.df))

r <- as(m, "realRatingMatrix")

```

1 Create a normalized rating matrix, `r_z`, using $Z$-scores. Inspect the raw rating and Z-normalized ratings using `image()`.
```{r}
## Put your R code here.
#Normalization tries to reduce the individual rating bias by row centering the data, i.e., by subtracting from each available rating the mean of the ratings of that user (row). Z-score in addition divides by the standard deviation of the row/column. Normalization can also be done on columns.
r_norm <- normalize(r)
r_norm_image <- image(r_norm,main = "norm ");r_norm_image 
r_z <- normalize(r, method="Z-score")
r_rawrating <- image(r, main = "Raw Ratings");r_rawrating
r_z_image <- image(r_z, main = "ZScore Ratings");r_z_image
```

2 Plot the raw and Z-score normalized rating.
```{r}
## Put your R code here.
hist(getRatings(r), breaks=100)
#hist(getRatings(r_norm), breaks=100)
hist(getRatings(r_z), breaks=100)
```

3 Plot the raw user counts and raw item means. Discuss.
```{r}
## Put your R code here.
head(as(r, "list"))
rowCounts(r)
colMeans(r)
#head(as(r_norm, "list"))
#rowCounts(r_norm[,])
#colMeans(r_norm[,])
hist(rowCounts(r), breaks=50)
hist(colMeans(r), breaks=50)
boxplot(rowCounts(r))
boxplot(colMeans(r))
# number of items that has been bought are almost equally distributed, around 8.
# The mean rate for each item is almost the around 3 
```

4 Create a recommender based on the popularity of items for the first 15 users using the raw ratings.
```{r}
## Put your R code here.
recommenderRegistry$get_entries(dataType = "realRatingMatrix")
r_pop <- Recommender(r[1:15], method = "POPULAR")
names(getModel(r_pop))
r_pop_get <- getModel(r_pop)
recom <- predict(r_pop, r[1:15], n=5) # I didn't know anyother way to show the output other than predicting based on the training data itself.
as(recom,"list")
```

5 Create the top-5 recommendation lists for the last five users.
```{r}
## Put your R code here.
r_lastpop <- Recommender(r[(length(rownames(r))-4):length(rownames(r))], method = "POPULAR") # if this question meant that based on the first 15 data, find the recommendations for the last five then I would have used this line  instead r_lastpop <- Recommender(r[1:(length(rownames(r))-5)], method = "POPULAR")

#r_lastpop; getModel(r_lastpop)
r_last_recom <- predict(r_lastpop, r[(length(rownames(r))-4):length(rownames(r))] )
as(r_last_recom,"list") 
r_last_best <- bestN(r_last_recom,n=5)
as(r_last_best,"list")

```

6 Create an evaluation scheme which splits the 20 users into a training set (75%) and a test set (25%). For the test set 5 items will be given to the recommender algorithm and the other items will be held out for computing the error. Use a `goodRating` value of 3.
```{r}
## Put your R code here.
e <- evaluationScheme(r, method="split", train=0.75, given=5, goodRating=3);e
# train.r <- r[sample,]
# test.r <- r[-sample,]
# r.train <- Recommender(train.r[1:5], method = "POPULAR")
# recom <- predict(r.train, train.r[6:15] ,n=5)
# as(recom, "list")
# e <- evaluationScheme(test.r, method="split", train=0.75, given=5, goodRating=3);e
```

7 Create two recommenders (user-based "UBCF" and item-based collaborative filtering "IBCF"") using the training data. Compute predicted ratings for the known part of the test data (5 items for each user) using the two algorithms.  Calculate the error (RMSE, MSE, and MAE) between the prediction and the unknown part of the test data. Discuss.
```{r}
## Put your R code here.
r1 <- Recommender(getData(e, "train"), "UBCF");r1 #How can I take a look at its values? as(r,"list") is not working
r2 <- Recommender(getData(e, "train"), "IBCF")
p1 <- predict(r1, getData(e, "known"), type="ratings")
p2 <- predict(r2, getData(e, "known"), type="ratings")
error <- rbind(calcPredictionAccuracy(p1, getData(e, "unknown")),calcPredictionAccuracy(p2,getData(e,"unknown")))
rownames(error) <- c("UBCF","IBCF")
error
# In this case IBCF produces smaller prediction error
```

8 create a 5-fold cross validation scheme with the the Given-5 protocol,
i.e., for the test users all but five randomly selected items are withheld for evaluation. Compute the average confusion matrix.
```{r}
## Put your R code here.
scheme <- evaluationScheme(r, method="cross", k=5, given=5,goodRating=3) # when I set given to 5 then confusion matrix get incorrect
scheme
sample5 <- sample(1:20, .75*20)
results <- evaluate(scheme, method="POPULAR", n=sort(sample(sample5,5)))
results
getConfusionMatrix(results)[[1]]
avg(results)
```

9 Plot the ROC curve and the precision-recall plot. Discuss.
```{r}
## Put your R code here.
#library(ROCR) # I was firstly think that I need to use this library but then I figured out that ROC is nothing but TPR vs FPR
plot(results, annotate=TRUE)
# from the figure we could see that the user based is almost the same for different cases while the popular methods and random items gets better. for the plots we could see that popular items are better at most of the cases the the other two.

plot(results, "prec/rec", annotate=TRUE)
#Here we see that the precision( TP / ( TP+FP)) over recall ( TP / (TP+FN)) in increased first but then it falls down.
```

10 Use the evaluation scheme created above to compare the three recommender algorithms: random items, popular items, and user-based CF based recommendations. Plot the ROC curve and the precision-recall plot with the three methods on each plot. Discuss.
```{r}
## Put your R code here.
algorithms <- list(
  "random items" = list(name="RANDOM", param=NULL),
  "popular items" = list(name="POPULAR", param=NULL),
  "user-based CF" = list(name="UBCF", param=list(method="Cosine",
        nn=50, minRating=5))
  )
results <- evaluate(scheme, algorithms, n=sort(sample(sample5,6)))
plot(results, annotate=c(1,3), legend="topleft")
# The figure shows that the popular items method and random items method works better than the user-based method because they TPR / FPR are bigger for them.
```
